{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YSJ_MuKSRJN"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWSZYEt_1j-s"
   },
   "outputs": [],
   "source": [
    "def onehot(vals, possible_vals):\n",
    "    if not isinstance(possible_vals, list): raise TypeError(\"provide possible_vals as a list\")\n",
    "    enc_vals = np.zeros([len(vals), len(possible_vals)])\n",
    "    for i, value in enumerate(vals):\n",
    "        if isinstance(possible_vals[0], float):\n",
    "            enc = np.where(abs(possible_vals-value)<1e-3)\n",
    "        else:\n",
    "            enc = np.where(possible_vals==value)\n",
    "        enc_vals[i,enc] = 1\n",
    "    return enc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5kdFmDunl-3"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def __call__(self, im):\n",
    "        w, h = [int(s*self.scalar) for s in im.size]\n",
    "        return transforms.Resize((h, w))(im)\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, box):\n",
    "        assert len(box) == 4\n",
    "        self.box = box\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return im.crop(self.box)\n",
    "\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def get_data_transforms(t='train'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWlWnMvlY2is"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#LABEL_KEYS = ['v_throttle', 'v_break', 'v_steer', 'traffic_light', 'front_vehicle', 'centre_dist', 'pedestrian_distance']\n",
    "\n",
    "class CARLA_Dataset(Dataset):\n",
    "    def __init__(self, t, pickle_file_path_l, image_dir_path, pickle_file_path_r):\n",
    "        #self.inputs, self.labels = {}, {}\n",
    "        #initializing transforms\n",
    "        assert t in ['train', 'val']\n",
    "        self.transform = get_data_transforms(t)\n",
    "        self.labels = {}    \n",
    "        ###############LEFT CAMERA##############\n",
    "        #reading the title of all images to access the pickle file\n",
    "        self.image_path = image_dir_path\n",
    "        self.image_list = os.listdir(image_dir_path)\n",
    "        #self.image_list_l.remove('.DS_Store')\n",
    "        self.file_name = []\n",
    "        for file in self.image_list:\n",
    "            self.file_name.append(os.path.splitext(file)[0])\n",
    "            \n",
    "        #initialize the labels \n",
    "\n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_l = os.listdir(pickle_file_path_l)\n",
    "        self.pickled_data_l = {}\n",
    "        for file in self.pickle_list_l:\n",
    "            f = open((pickle_file_path_l + file), 'rb')  \n",
    "            self.pickled_data_l.update(pickle.load(f))\n",
    "            f.close() \n",
    "        \n",
    "        ###############RIGHT CAMERA##############\n",
    "    \n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_r = os.listdir(pickle_file_path_r)\n",
    "        self.pickled_data_r = {}\n",
    "        for file in self.pickle_list_r:\n",
    "            f = open((pickle_file_path_r + file), 'rb')  \n",
    "            self.pickled_data_r.update(pickle.load(f))\n",
    "            f.close() \n",
    "        \n",
    "        # transform reg affordances\n",
    "        #reg_keys = ['front_vehicle', 'centre_dist', 'pedestrian_distance']\n",
    "        #for key in self.pickled_data:\n",
    "        #   entry = self.pickled_data[key]\n",
    "            #print(\"1.\", entry['front_vehicle'])\n",
    "        #reg_norm = entry[reg_keys] / abs(entry[reg_keys]).max()\n",
    "                # transform cls affordances\n",
    "      \n",
    "        f.close() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames      = []\n",
    "        inputs = {}\n",
    "            \n",
    "        #####################LEFT CAMERA################\n",
    "        #reading PIL\n",
    "        self.raw_image = Image.open(os.path.join(self.image_path, self.image_list[idx])).convert('RGB')\n",
    "        #pyplot.imshow(self.raw_image_l)\n",
    "        #pyplot.show()\n",
    "        \n",
    "        #reading file name to access the pickle file\n",
    "        current_fname = self.file_name[idx]\n",
    "        chk_camera = current_fname[-3:]\n",
    "\n",
    "        if chk_camera == '_rt' :\n",
    "            current_fname_r = current_fname[:-3]\n",
    "            label_dict = self.pickled_data_r[current_fname_r] \n",
    "\n",
    "        else :\n",
    "\n",
    "            label_dict = self.pickled_data_l[current_fname] \n",
    "            \n",
    "        label_values = list(label_dict.values())\n",
    "        \n",
    "        #transforming regression labels\n",
    "        self.labels['front_vehicle'] = torch.Tensor(np.array(float(label_dict['front_vehicle'])))\n",
    "        self.labels['centre_dist'] = torch.Tensor(np.array(float(label_dict['centre_dist'])))\n",
    "        self.labels['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict['pedestrian_distance'])))\n",
    "        \n",
    "        #transforming classification labels\n",
    "        #self.labels_l['traffic_light'] = torch.Tensor(onehot(np.array(label_dict['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "        #transforming raw PIL image\n",
    "        im = self.transform(self.raw_image)\n",
    "        \n",
    "        inputs ['sequence'] = im\n",
    "        \n",
    "        #print(inputs ['sequence'].dim())\n",
    "        return inputs, self.labels\n",
    "\n",
    "            \n",
    "        ###############RIGHT CAMERA####################\n",
    "        #reading PIL\n",
    "        #self.raw_image_r = Image.open(os.path.join(self.image_path_r, self.image_list_r[idx])).convert('RGB')\n",
    "        #pyplot.imshow(self.raw_image_r)\n",
    "        #pyplot.show()\n",
    "        \n",
    "        #reading file name to access the pickle file\n",
    "        #current_fname_r = self.file_name_r[idx]\n",
    "        #label_dict_r = self.pickled_data_r[current_fname_r]       \n",
    "        #label_values_r = list(label_dict_r.values())\n",
    "       \n",
    "        #transforming regression labels\n",
    "        #self.labels_r['front_vehicle'] = torch.Tensor(np.array(float(label_dict_r['front_vehicle'])))\n",
    "        #self.labels_r['centre_dist'] = torch.Tensor(np.array(float(label_dict_r['centre_dist'])))\n",
    "        #self.labels_r['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict_r['pedestrian_distance'])))\n",
    "        \n",
    "        #transforming classification labels\n",
    "       # self.labels_r['traffic_light'] = torch.Tensor(onehot(np.array(label_dict_r['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "        \n",
    "        #transforming raw PIL image\n",
    "       # im_r = self.transform(self.raw_image_r)\n",
    "        \n",
    "        #inputs ['sequence'] = im_r\n",
    "            \n",
    "        #return transformed image and labels\n",
    "        #sample = {'image_l' : im_l, 'labels_l' : self.labels_l ,'image_r' : im_r, 'labels_r' : self.labels_r}\n",
    "        #return sample\n",
    "\n",
    "        \n",
    "        #self.labels['v_break'] = torch.Tensor(onehot(np.array(list(label_dict['v_break'])), [0.0, 1.0]))\n",
    "        #print(label_dict['traffic_light'])\n",
    "        #print(self.labels['traffic_light'])\n",
    "        #print(label_dict['v_break'])\n",
    "        #print(self.labels['v_break'])\n",
    "        #pyplot.imshow(self.raw_image)\n",
    "        #pyplot.show()\n",
    "        #reg_keys = ('front_vehicle', 'centre_dist', 'pedestrian_distance', 'v_steer')\n",
    "        #reg_norm = label_dict[reg_keys] / abs(label_dict[reg_keys]).max()\n",
    "        #reg_norm = label_dict[reg_keys]\n",
    "\n",
    "        #for i in 'front_vehicle', 'centre_dist', 'pedestrian_distance', 'v_steer':\n",
    "        #  reg_norm = label_dict[i] / abs(label_dict[i])\n",
    "        #  print(i, reg_norm)\n",
    "        #print(type(reg_keys))\n",
    "        #reg_norm = label_dict['front_vehicle', 'centre_dist'] / max(abs(label_dict['front_vehicle', 'centre_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1572580450204,
     "user": {
      "displayName": "Seima Saki",
      "photoUrl": "",
      "userId": "02501578531897632617"
     },
     "user_tz": -480
    },
    "id": "pg-IJ8D-_x1S",
    "outputId": "657a685a-8200-44e4-d42f-6db08adee6eb"
   },
   "outputs": [],
   "source": [
    "#C_dataset = CARLA_Dataset( 'train', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/', image_dir_path='/Users/seimasaki/CE7454_2019/codes/data/Left_camera/', pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/')\n",
    "\n",
    "#for i in range(len(C_dataset)):\n",
    "  #  sample = C_dataset[i]\n",
    "   # print(sample[0])\n",
    " #print(sample['labels_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size):\n",
    "    train_ds = CARLA_Dataset( 'train', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/',\n",
    "                                    image_dir_path='/Users/seimasaki/CE7454_2019/codes/data/Camera/', \n",
    "                                    pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/')\n",
    "    val_ds = CARLA_Dataset( 'val', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/',\n",
    "                                    image_dir_path='/Users/seimasaki/CE7454_2019/codes/data/Camera/', \n",
    "                                    pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/')\n",
    "    return (len(train_ds),len(val_ds),DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=10),\n",
    "        DataLoader(val_ds, batch_size=batch_size*2, pin_memory=True, num_workers=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_data(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# pytorch util functions \n",
    "##########################################################\n",
    "\n",
    "def load_checkpoint(filename) :\n",
    "    #print(filename)\n",
    "    checkpoint = torch.load(filename)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return [model_state_dict, optimizer_state_dict, epoch, loss]\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    print(\"Saving Checkpoint....\")\n",
    "    model_state_dict = model.state_dict()\n",
    "    optimizer_state_dict = optimizer.state_dict()\n",
    "#     print(\"model_state_dict : \", model_state_dict)\n",
    "#     print(\"optimizer_state_dict : \", optimizer_state_dict)\n",
    "#     print(\"Epoch : \", epoch)\n",
    "#     print(\"filename : \", filename)\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'loss': loss\n",
    "            }, filename)\n",
    "    print(\"Saving Checkpoint Done\")\n",
    "\n",
    "def display_image(image) : \n",
    "    plt.show(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "#from ipdb import set_trace\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from util_funcs import *\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../prev_work/CAL/training\")\n",
    "\n",
    "#from train import fit, custom_loss, validate\n",
    "from metrics import calc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs(inputs_old, labels):\n",
    "    inputs            = inputs_old['sequence']\n",
    "    #print(\"labels keys : \", labels.keys())\n",
    "    #labels = list(sample['labels_l'].values())\n",
    "    labels_vdistance  = labels['front_vehicle']\n",
    "    labels_cdistance  = labels['centre_dist']\n",
    "    labels_direction  = labels['pedestrian_distance']\n",
    "    combined_label    = torch.stack([labels_vdistance, labels_cdistance, labels_direction], dim=1)\n",
    "    #print(\"combined_label shape : \", combined_label.size())\n",
    "    return inputs, combined_label\n",
    "    #return combined_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #for inputs, labels in dataloaders[phase]:\n",
    "            for i, data in enumerate(dataloaders[phase]) :\n",
    "            #for i, data in tqdm(dataloaders[phase]) :\n",
    "                inputs_old = data[0]\n",
    "                #print(\"input before convertion\", inputs_old)\n",
    "                labels = data[1]\n",
    "                inputs, labels = convert_inputs(inputs_old ,labels)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    preds = outputs\n",
    "                    print(outputs.size(), labels.size())\n",
    "                    #input(\"Enter\")\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if(i%100 == 0) :\n",
    "                            print(\"preds shape : \", preds.size(), \" labels shape : \", labels.size())\n",
    "                            print(\"preds : \", preds[0].T.data, \"\\nlabels : \", labels.data[0].T.data)\n",
    "                            print(\"loss :\", loss.item())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if(phase == 'val') :\n",
    "                    if(i%10==0) :\n",
    "                        print(\"preds : \", preds[0].T.data, \"\\nlabels : \", labels.data[0].T.data)\n",
    "                        print(\"loss :\", loss.item())\n",
    "                #if(i==10) : \n",
    "                #    break\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc > best_acc:\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                #best_acc = epoch_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'train'  :\n",
    "                checkpoint_name = 'checkpoints/model_' + str(epoch) + '.tar'\n",
    "                save_checkpoint(model, optimizer, epoch, epoch_loss, checkpoint_name)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_weights(checkpoint, model) :\n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "\n",
    "def resume_model(checkpoint, model, optimizer) : \n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    #print(_model_state_dict)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "    optimizer.load_state_dict(_optimizer_state_dict)\n",
    "    print(\"resume model succesful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x128af8dd8>\n"
     ]
    }
   ],
   "source": [
    "len_train_ds, len_valid_ds, train_dl, valid_dl = get_data(batch_size=128)\n",
    "print(len_train_ds)\n",
    "print(len_valid_ds)\n",
    "\n",
    "dataloaders   = {'train':train_dl, 'val':valid_dl}\n",
    "print(dataloaders['train'])\n",
    "dataset_sizes = {'train':len_train_ds, 'val':len_valid_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sizes : {'train': 48, 'val': 48}\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset sizes :\", dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying model to device.... cpu\n"
     ]
    }
   ],
   "source": [
    "#model definitions\n",
    "model_ft         = models.resnet50(pretrained=True)\n",
    "criterion        = nn.MSELoss()\n",
    "optimizer_ft     = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#load_checkpoint_weights(args.checkpoint, model_ft)\n",
    "num_ftrs         = model_ft.fc.in_features\n",
    "model_ft.fc      = nn.Linear(num_ftrs, 3)\n",
    "model_ft         = nn.DataParallel(model_ft)\n",
    "model_ft         = model_ft.to(device)\n",
    "print(\"copying model to device....\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch 0/1\n",
      "----------\n",
      "torch.Size([48, 3]) torch.Size([48, 3])\n",
      "preds shape :  torch.Size([48, 3])  labels shape :  torch.Size([48, 3])\n",
      "preds :  tensor([ 0.1922, -0.1138, -0.3542]) \n",
      "labels :  tensor([50.0000,  0.4116, 50.0000])\n",
      "loss : 1679.7928466796875\n",
      "train Loss: 1679.7928 Acc: 0.0000\n",
      "Saving Checkpoint....\n",
      "Saving Checkpoint Done\n",
      "torch.Size([48, 3]) torch.Size([48, 3])\n",
      "preds :  tensor([ 0.2485,  0.2698, -0.5820]) \n",
      "labels :  tensor([50.0000,  0.6656, 50.0000])\n",
      "loss : 1681.686767578125\n",
      "val Loss: 1681.6868 Acc: 0.0000\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "torch.Size([48, 3]) torch.Size([48, 3])\n",
      "preds shape :  torch.Size([48, 3])  labels shape :  torch.Size([48, 3])\n",
      "preds :  tensor([ 0.2457, -0.2158, -0.4988]) \n",
      "labels :  tensor([50.0000,  0.4116, 50.0000])\n",
      "loss : 1672.541259765625\n",
      "train Loss: 1672.5413 Acc: 0.0000\n",
      "Saving Checkpoint....\n",
      "Saving Checkpoint Done\n",
      "torch.Size([48, 3]) torch.Size([48, 3])\n",
      "preds :  tensor([ 0.2559,  0.2406, -0.5408]) \n",
      "labels :  tensor([50.0000,  0.6656, 50.0000])\n",
      "loss : 1679.6414794921875\n",
      "val Loss: 1679.6415 Acc: 0.0000\n",
      "\n",
      "Training complete in 2m 56s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "print(\"starting training...\")\n",
    "#if(args.retrain == True) : \n",
    " #   resume_model(args.checkpoint, model_ft, optimizer_ft)\n",
    "train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_loader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
