{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YSJ_MuKSRJN"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWSZYEt_1j-s"
   },
   "outputs": [],
   "source": [
    "def onehot(vals, possible_vals):\n",
    "    if not isinstance(possible_vals, list): raise TypeError(\"provide possible_vals as a list\")\n",
    "    enc_vals = np.zeros([len(vals), len(possible_vals)])\n",
    "    for i, value in enumerate(vals):\n",
    "        if isinstance(possible_vals[0], float):\n",
    "            enc = np.where(abs(possible_vals-value)<1e-3)\n",
    "        else:\n",
    "            enc = np.where(possible_vals==value)\n",
    "        enc_vals[i,enc] = 1\n",
    "    return enc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5kdFmDunl-3"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def __call__(self, im):\n",
    "        w, h = [int(s*self.scalar) for s in im.size]\n",
    "        return transforms.Resize((h, w))(im)\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, box):\n",
    "        assert len(box) == 4\n",
    "        self.box = box\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return im.crop(self.box)\n",
    "\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def get_data_transforms(t='train'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWlWnMvlY2is"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#LABEL_KEYS = ['v_throttle', 'v_break', 'v_steer', 'traffic_light', 'front_vehicle', 'centre_dist', 'pedestrian_distance']\n",
    "\n",
    "class CARLA_Dataset(Dataset):\n",
    "    def __init__(self, t, pickle_file_path_l, image_dir_path_l, pickle_file_path_r, image_dir_path_r):\n",
    "        self.inputs, self.labels = {}, {}\n",
    "        #initializing transforms\n",
    "        assert t in ['train', 'val']\n",
    "        self.transform = get_data_transforms(t)\n",
    "        \n",
    "        ###############LEFT CAMERA##############\n",
    "        #reading the title of all images to access the pickle file\n",
    "        self.image_path_l = image_dir_path_l\n",
    "        self.image_list_l = os.listdir(image_dir_path_l)\n",
    "        self.image_list_l.remove('.DS_Store')\n",
    "        self.file_name_l = []\n",
    "        for file in self.image_list_l:\n",
    "            self.file_name_l.append(os.path.splitext(file)[0])\n",
    "            \n",
    "        #initialize the labels which should be returned\n",
    "        self.labels_l = {}\n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_l = os.listdir(pickle_file_path_l)\n",
    "        self.pickled_data_l = {}\n",
    "        for file in self.pickle_list_l:\n",
    "            f = open((pickle_file_path_l + file), 'rb')  \n",
    "            self.pickled_data_l.update(pickle.load(f))\n",
    "            f.close() \n",
    "        \n",
    "        ###############RIGHT CAMERA##############\n",
    "        #reading the title of all images to access the pickle file\n",
    "        self.image_path_r = image_dir_path_r\n",
    "        self.image_list_r = os.listdir(image_dir_path_r)\n",
    "        self.image_list_r.remove('.DS_Store')\n",
    "        self.file_name_r = []\n",
    "        for file in self.image_list_r:\n",
    "            #self.file_name.append(file[0:16])\n",
    "            self.file_name_r.append(os.path.splitext(file)[0])\n",
    "            \n",
    "        #initialize the labels which should be returned\n",
    "        self.labels_r = {}\n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_r = os.listdir(pickle_file_path_r)\n",
    "        self.pickled_data_r = {}\n",
    "        for file in self.pickle_list_r:\n",
    "            f = open((pickle_file_path_r + file), 'rb')  \n",
    "            self.pickled_data_r.update(pickle.load(f))\n",
    "            f.close() \n",
    "        \n",
    "        # transform reg affordances\n",
    "        #reg_keys = ['front_vehicle', 'centre_dist', 'pedestrian_distance']\n",
    "        #for key in self.pickled_data:\n",
    "        #   entry = self.pickled_data[key]\n",
    "            #print(\"1.\", entry['front_vehicle'])\n",
    "        #reg_norm = entry[reg_keys] / abs(entry[reg_keys]).max()\n",
    "                # transform cls affordances\n",
    "      \n",
    "        f.close() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name_l)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames      = []\n",
    "        #####################LEFT CAMERA################\n",
    "        #reading PIL\n",
    "        self.raw_image_l = Image.open(os.path.join(self.image_path_l, self.image_list_l[idx]))\n",
    "        #pyplot.imshow(self.raw_image_l)\n",
    "        #pyplot.show()\n",
    "        \n",
    "        #reading file name to access the pickle file\n",
    "        current_fname_l = self.file_name_l[idx]\n",
    "        label_dict_l = self.pickled_data_l[current_fname_l]       \n",
    "        label_values_l = list(label_dict_l.values())\n",
    "        \n",
    "        #transforming regression labels\n",
    "        self.labels_l['front_vehicle'] = torch.Tensor(np.array(float(label_dict_l['front_vehicle'])))\n",
    "        self.labels_l['centre_dist'] = torch.Tensor(np.array(float(label_dict_l['centre_dist'])))\n",
    "        self.labels_l['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict_l['pedestrian_distance'])))\n",
    "        \n",
    "        #transforming classification labels\n",
    "        self.labels_l['traffic_light'] = torch.Tensor(onehot(np.array(label_dict_l['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "        #transforming raw PIL image\n",
    "        im_l = self.transform(self.raw_image_l)\n",
    "        \n",
    "        ###############RIGHT CAMERA####################\n",
    "        #reading PIL\n",
    "        self.raw_image_r = Image.open(os.path.join(self.image_path_r, self.image_list_r[idx]))\n",
    "        #pyplot.imshow(self.raw_image_r)\n",
    "        #pyplot.show()\n",
    "        \n",
    "        #reading file name to access the pickle file\n",
    "        current_fname_r = self.file_name_r[idx]\n",
    "        label_dict_r = self.pickled_data_r[current_fname_r]       \n",
    "        label_values_r = list(label_dict_r.values())\n",
    "       \n",
    "        #transforming regression labels\n",
    "        self.labels_r['front_vehicle'] = torch.Tensor(np.array(float(label_dict_r['front_vehicle'])))\n",
    "        self.labels_r['centre_dist'] = torch.Tensor(np.array(float(label_dict_r['centre_dist'])))\n",
    "        self.labels_r['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict_r['pedestrian_distance'])))\n",
    "        \n",
    "        #transforming classification labels\n",
    "        self.labels_r['traffic_light'] = torch.Tensor(onehot(np.array(label_dict_r['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "        \n",
    "        #transforming raw PIL image\n",
    "        im_r = self.transform(self.raw_image_r)\n",
    "        \n",
    "        #return transformed image and labels\n",
    "        sample = {'image_l' : im_l, 'labels_l' : self.labels_l ,'image_r' : im_r, 'labels_r' : self.labels_r}\n",
    "        return sample\n",
    "        \n",
    "        #self.labels['v_break'] = torch.Tensor(onehot(np.array(list(label_dict['v_break'])), [0.0, 1.0]))\n",
    "        #print(label_dict['traffic_light'])\n",
    "        #print(self.labels['traffic_light'])\n",
    "        #print(label_dict['v_break'])\n",
    "        #print(self.labels['v_break'])\n",
    "        #pyplot.imshow(self.raw_image)\n",
    "        #pyplot.show()\n",
    "        #reg_keys = ('front_vehicle', 'centre_dist', 'pedestrian_distance', 'v_steer')\n",
    "        #reg_norm = label_dict[reg_keys] / abs(label_dict[reg_keys]).max()\n",
    "        #reg_norm = label_dict[reg_keys]\n",
    "\n",
    "        #for i in 'front_vehicle', 'centre_dist', 'pedestrian_distance', 'v_steer':\n",
    "        #  reg_norm = label_dict[i] / abs(label_dict[i])\n",
    "        #  print(i, reg_norm)\n",
    "        #print(type(reg_keys))\n",
    "        #reg_norm = label_dict['front_vehicle', 'centre_dist'] / max(abs(label_dict['front_vehicle', 'centre_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1572580450204,
     "user": {
      "displayName": "Seima Saki",
      "photoUrl": "",
      "userId": "02501578531897632617"
     },
     "user_tz": -480
    },
    "id": "pg-IJ8D-_x1S",
    "outputId": "657a685a-8200-44e4-d42f-6db08adee6eb"
   },
   "outputs": [],
   "source": [
    "#C_dataset = CARLA_Dataset( 'train', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/', image_dir_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_camera/', pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/', image_dir_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_camera/', )\n",
    "\n",
    "#for i in range(len(C_dataset)):\n",
    " # sample = C_dataset[i]\n",
    "  #print(sample['image_l'])\n",
    " # print(sample['labels_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size):\n",
    "    train_ds = CARLA_Dataset( 'train', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/',\n",
    "                                    image_dir_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_camera/', \n",
    "                                    pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/',\n",
    "                                    image_dir_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_camera/', )\n",
    "    val_ds = CARLA_Dataset( 'val', pickle_file_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_pickle_file/',\n",
    "                                    image_dir_path_l='/Users/seimasaki/CE7454_2019/codes/data/Left_camera/', \n",
    "                                    pickle_file_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_pickle_file/',\n",
    "                                    image_dir_path_r='/Users/seimasaki/CE7454_2019/codes/data/Right_camera/', )\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=10),\n",
    "        #DataLoader(val_ds, batch_size=batch_size*2, pin_memory=True, num_workers=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<torch.utils.data.dataloader.DataLoader object at 0x125bb4128>, <torch.utils.data.dataloader.DataLoader object at 0x124dc8518>)\n"
     ]
    }
   ],
   "source": [
    "print(get_data(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_loader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
