{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "#from ipdb import set_trace\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import argparse\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(vals, possible_vals):\n",
    "    if not isinstance(possible_vals, list): raise TypeError(\"provide possible_vals as a list\")\n",
    "    enc_vals = np.zeros([len(vals), len(possible_vals)])\n",
    "    for i, value in enumerate(vals):\n",
    "        if isinstance(possible_vals[0], float):\n",
    "            enc = np.where(abs(possible_vals-value)<1e-3)\n",
    "        else:\n",
    "            enc = np.where(possible_vals==value)\n",
    "        enc_vals[i,enc] = 1\n",
    "    return enc_vals\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def __call__(self, im):\n",
    "        w, h = [int(s*self.scalar) for s in im.size]\n",
    "        return transforms.Resize((h, w))(im)\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, box):\n",
    "        assert len(box) == 4\n",
    "        self.box = box\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return im.crop(self.box)\n",
    "\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def load_checkpoint(filename) :\n",
    "    #print(filename)\n",
    "    checkpoint = torch.load(filename)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return [model_state_dict, optimizer_state_dict, epoch, loss]\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    print(\"Saving Checkpoint....\")\n",
    "    model_state_dict = model.state_dict()\n",
    "    optimizer_state_dict = optimizer.state_dict()\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'loss': loss\n",
    "            }, filename)\n",
    "    print(\"Saving Checkpoint Done\")\n",
    "\n",
    "def display_image(image) : \n",
    "    plt.show(image)\n",
    "    \n",
    "def load_checkpoint_weights(checkpoint, model) :\n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "\n",
    "def resume_model(checkpoint, model, optimizer) : \n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    #print(_model_state_dict)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "    optimizer.load_state_dict(_optimizer_state_dict)\n",
    "    print(\"resume model succesful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, data in enumerate(dataloaders[phase]) :\n",
    "                inputs_old = data[0]\n",
    "                #print(\"input before convertion\", inputs_old)\n",
    "                labels = data[1]\n",
    "                inputs, labels = convert_inputs(inputs_old ,labels)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    preds = outputs\n",
    "                    #print(outputs.size(), labels.size())\n",
    "                    #input(\"Enter\")\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if(i%100 == 0) :\n",
    "                            print(\"preds shape : \", preds.size(), \" labels shape : \", labels.size())\n",
    "                            print(\"preds : \", preds[0:5].T.data, \"\\nlabels : \", labels.data[0:5].T.data)\n",
    "                            print(\"loss :\", loss.item())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if(phase == 'val') :\n",
    "                    if(i%10==0) :\n",
    "                        print(\"preds : \", preds[0:5].T.data, \"\\nlabels : \", labels.data[0:5].T.data)\n",
    "                        print(\"loss :\", loss.item())\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc > best_acc:\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                #best_acc = epoch_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'train'  :\n",
    "                checkpoint_name = 'checkpoints/model_' + str(epoch) + '.tar'\n",
    "                save_checkpoint(model, optimizer, epoch, epoch_loss, checkpoint_name)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations():\n",
    "    # applies the given augmenter in 50% of all cases,\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "    # Define our sequence of augmentation steps that will be applied to every image\n",
    "    seq = iaa.Sequential([\n",
    "            # execute 0 to 5 of the following (less important) augmenters per image\n",
    "            iaa.SomeOf((0, 5),\n",
    "                [\n",
    "                    iaa.OneOf([\n",
    "                        iaa.GaussianBlur((0, 3.0)),\n",
    "                        iaa.AverageBlur(k=(2, 7)),\n",
    "                        iaa.MedianBlur(k=(3, 11)),\n",
    "                    ]),\n",
    "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
    "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
    "                    # search either for all edges or for directed edges,\n",
    "                    # blend the result with the original image using a blobby mask\n",
    "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                    ])),\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                        iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                    ]),\n",
    "                    iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                    iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "                    # either change the brightness of the whole image (sometimes\n",
    "                    # per channel) or change the brightness of subareas\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
    "                        iaa.FrequencyNoiseAlpha(\n",
    "                            exponent=(-4, 0),\n",
    "                            first=iaa.Multiply((0.5, 1.5), per_channel=True),\n",
    "                            second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "                        )\n",
    "                    ]),\n",
    "                    iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ],\n",
    "        random_order=True\n",
    "    )\n",
    "    return seq\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "class CARLA_Dataset(Dataset):\n",
    "    def __init__(self, t, pickle_file_path_l, image_dir_path, pickle_file_path_r):\n",
    "        #initializing transforms\n",
    "        assert t in ['train', 'val']\n",
    "        self.transform = get_data_transforms(t)\n",
    "#         selabelslf.labels = {}    \n",
    "        ###############LEFT CAMERA##############\n",
    "        #reading the title of all images to access the pickle file\n",
    "        self.image_path = image_dir_path\n",
    "        self.image_list = os.listdir(image_dir_path)\n",
    "        self.file_name = []\n",
    "        for file in self.image_list:\n",
    "            self.file_name.append(os.path.splitext(file)[0])\n",
    "            \n",
    "        #initialize the labels \n",
    "        #read pickle file\n",
    "        self.pickle_list_l = os.listdir(pickle_file_path_l)\n",
    "        self.pickled_data_l = {}\n",
    "        for file in self.pickle_list_l:\n",
    "            f = open((pickle_file_path_l + file), 'rb')  \n",
    "            self.pickled_data_l.update(pickle.load(f))\n",
    "            f.close() \n",
    "        \n",
    "        ###############RIGHT CAMERA##############\n",
    "        #read pickle file\n",
    "        self.pickle_list_r = os.listdir(pickle_file_path_r)\n",
    "        self.pickled_data_r = {}\n",
    "        for file in self.pickle_list_r:\n",
    "            f = open((pickle_file_path_r + file), 'rb')  \n",
    "            self.pickled_data_r.update(pickle.load(f))\n",
    "            f.close() \n",
    "        f.close() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames      = []\n",
    "        inputs = {}  \n",
    "        labels = {}\n",
    "        #####################LEFT CAMERA################\n",
    "        #reading PIL\n",
    "#         self.raw_image = Image.open(os.path.join(self.image_path, self.image_list[idx])).convert('RGB')\n",
    "        raw_image = Image.open(os.path.join(self.image_path, self.image_list[idx])).convert('RGB')\n",
    "        #pyplot.imshow(self.raw_image_l)\n",
    "        #pyplot.show()\n",
    "        \n",
    "        #reading file name to access the pickle file\n",
    "        current_fname = self.file_name[idx]\n",
    "#         print(current_fname)\n",
    "        chk_camera = current_fname[-3:]\n",
    "\n",
    "        if chk_camera == '_rt' :\n",
    "            current_fname_r = current_fname[:-3]\n",
    "            label_dict = self.pickled_data_r[current_fname_r] \n",
    "        else :\n",
    "            label_dict = self.pickled_data_l[current_fname] \n",
    "        label_values = list(label_dict.values())\n",
    "#         print(label_values)\n",
    "        #transforming regression labels\n",
    "        labels['front_vehicle'] = torch.Tensor(np.array(float(label_dict['front_vehicle'])))\n",
    "        labels['centre_dist'] = torch.Tensor(np.array(float(label_dict['centre_dist'])))\n",
    "        labels['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict['pedestrian_distance'])))\n",
    "        \n",
    "#         print(labels['front_vehicle'])\n",
    "        #transforming classification labels\n",
    "        #self.labels_l['traffic_light'] = torch.Tensor(onehot(np.array(label_dict['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "        #transforming raw PIL image\n",
    "        im = self.transform(raw_image)\n",
    "        \n",
    "        inputs ['sequence'] = im\n",
    "#         inputs ['raw_image'] = self.raw_image\n",
    "        #print(inputs ['sequence'].dim())\n",
    "        return inputs, labels\n",
    "\n",
    "#dataset support functions\n",
    "def get_data_transforms(t='train'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "#             Augment(get_augmentations()),\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms[t]\n",
    "\n",
    "def get_data(batch_size):\n",
    "    train_ds = CARLA_Dataset( 'train', pickle_file_path_l='Training_data/Left_pickle_file/',\n",
    "                                    image_dir_path='Training_data/Camera/', \n",
    "                                    pickle_file_path_r='Training_data/Right_pickle_file/')\n",
    "    val_ds = CARLA_Dataset( 'val', pickle_file_path_l='Training_data/Left_pickle_file/',\n",
    "                                    image_dir_path='Training_data/Camera/', \n",
    "                                    pickle_file_path_r='Training_data/Right_pickle_file/')\n",
    "#     return (len(train_ds),len(val_ds),DataLoader(train_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=10),\n",
    "#         DataLoader(val_ds, batch_size=batch_size*2, pin_memory=True, num_workers=10),\n",
    "#     )\n",
    "    return (len(train_ds),len(val_ds),DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=8),\n",
    "        DataLoader(val_ds, batch_size=batch_size*2, num_workers=1),\n",
    "    )\n",
    "\n",
    "def convert_inputs(inputs_old, labels):\n",
    "    inputs            = inputs_old['sequence']\n",
    "    labels_vdistance  = labels['front_vehicle']\n",
    "    labels_cdistance  = labels['centre_dist']\n",
    "    labels_direction  = labels['pedestrian_distance']\n",
    "#     combined_label    = torch.stack([labels_vdistance, labels_cdistance, labels_direction], dim=1)\n",
    "#     combined_label    = torch.stack(labels_vdistance, dim=1)\n",
    "    combined_label = (labels_vdistance/50).unsqueeze(dim=1)\n",
    "#     print(\"converted labels : \",combined_label.size())\n",
    "    return inputs, combined_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_ds, len_valid_ds, train_dl, valid_dl = get_data(batch_size=64)\n",
    "print(len_train_ds)\n",
    "print(len_valid_ds)\n",
    "\n",
    "dataloaders   = {'train':train_dl, 'val':valid_dl}\n",
    "print(dataloaders['train'])\n",
    "dataset_sizes = {'train':len_train_ds, 'val':len_valid_ds}\n",
    "print(\"dataset sizes :\", dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def plot_tensor_image(tensor_im) :\n",
    "#     plt.imshow(tensor_im.permute(1, 2, 0))\n",
    "#     plt.show()\n",
    "\n",
    "# train_dataloader = dataloaders['train']\n",
    "# data             = next(iter(train_dataloader))\n",
    "# test_input , test_train_label = data\n",
    "# convert_inputs(test_input, test_train_label)\n",
    "\n",
    "# test_train_im = test_input['sequence']\n",
    "# print(test_train_label)\n",
    "# print(test_train_im.size())\n",
    "# for i in range(16) :\n",
    "#     print(test_train_label['front_vehicle'][i])\n",
    "#     plot_tensor_image(test_train_im[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definitions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft         = models.resnet50(pretrained=True)\n",
    "criterion        = nn.MSELoss()\n",
    "optimizer_ft     = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "num_ftrs         = model_ft.fc.in_features\n",
    "model_ft.fc      = nn.Linear(num_ftrs, 1)\n",
    "model_ft         = nn.DataParallel(model_ft)\n",
    "model_ft         = model_ft.to(device)\n",
    "load_checkpoint_weights('../python/checkpoints/vehicle_distance/model_19.tar', model_ft)\n",
    "print(\"copying model to device....\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #train model\n",
    "print(\"starting training...\")\n",
    "# #if(args.retrain == True) : \n",
    "#  #   resume_model(args.checkpoint, model_ft, optimizer_ft)\n",
    "train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
